{
  "description": "DNSSpec is the specification of the desired behavior of the DNS.",
  "type": "object",
  "properties": {
    "cache": {
      "description": "DNSCache defines the fields for configuring DNS caching.",
      "type": [
        "object",
        "null"
      ],
      "properties": {
        "negativeTTL": {
          "description": "Duration is a wrapper around time.Duration which supports correct marshaling to YAML and JSON. In particular, it marshals into strings, which can be used as map keys in json.",
          "type": [
            "string",
            "null"
          ]
        },
        "positiveTTL": {
          "description": "Duration is a wrapper around time.Duration which supports correct marshaling to YAML and JSON. In particular, it marshals into strings, which can be used as map keys in json.",
          "type": [
            "string",
            "null"
          ]
        }
      }
    },
    "logLevel": {
      "description": "logLevel describes the desired logging verbosity for CoreDNS. Any one of the following values may be specified: * Normal logs errors from upstream resolvers. * Debug logs errors, NXDOMAIN responses, and NODATA responses. * Trace logs errors and all responses.\n Setting logLevel: Trace will produce extremely verbose logs.\nValid values are: \"Normal\", \"Debug\", \"Trace\". Defaults to \"Normal\".",
      "type": [
        "string",
        "null"
      ]
    },
    "managementState": {
      "description": "managementState indicates whether the DNS operator should manage cluster DNS",
      "type": [
        "string",
        "null"
      ]
    },
    "nodePlacement": {
      "description": "DNSNodePlacement describes the node scheduling configuration for DNS pods.",
      "type": [
        "object",
        "null"
      ],
      "properties": {
        "nodeSelector": {
          "description": "nodeSelector is the node selector applied to DNS pods.\n\nIf empty, the default is used, which is currently the following:\n\n  kubernetes.io/os: linux\n\nThis default is subject to change.\n\nIf set, the specified selector is used and replaces the default.",
          "type": [
            "object",
            "null"
          ],
          "additionalProperties": {
            "type": [
              "string",
              "null"
            ],
            "default": ""
          }
        },
        "tolerations": {
          "description": "tolerations is a list of tolerations applied to DNS pods.\n\nIf empty, the DNS operator sets a toleration for the \"node-role.kubernetes.io/master\" taint.  This default is subject to change.  Specifying tolerations without including a toleration for the \"node-role.kubernetes.io/master\" taint may be risky as it could lead to an outage if all worker nodes become unavailable.\n\nNote that the daemon controller adds some tolerations as well.  See https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/",
          "type": [
            "array",
            "null"
          ],
          "items": {
            "description": "The pod this Toleration is attached to tolerates any taint that matches the triple <key,value,effect> using the matching operator <operator>.",
            "type": [
              "object",
              "null"
            ],
            "properties": {
              "effect": {
                "description": "Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute.\n\nPossible enum values:\n - `\"NoExecute\"` Evict any already-running pods that do not tolerate the taint. Currently enforced by NodeController.\n - `\"NoSchedule\"` Do not allow new pods to schedule onto the node unless they tolerate the taint, but allow all pods submitted to Kubelet without going through the scheduler to start, and allow all already-running pods to continue running. Enforced by the scheduler.\n - `\"PreferNoSchedule\"` Like TaintEffectNoSchedule, but the scheduler tries not to schedule new pods onto the node, rather than prohibiting new pods from scheduling onto the node entirely. Enforced by the scheduler.",
                "type": [
                  "string",
                  "null"
                ],
                "enum": [
                  "NoExecute",
                  "NoSchedule",
                  "PreferNoSchedule"
                ]
              },
              "key": {
                "description": "Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys.",
                "type": [
                  "string",
                  "null"
                ]
              },
              "operator": {
                "description": "Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category.\n\nPossible enum values:\n - `\"Equal\"`\n - `\"Exists\"`",
                "type": [
                  "string",
                  "null"
                ],
                "enum": [
                  "Equal",
                  "Exists"
                ]
              },
              "tolerationSeconds": {
                "description": "TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system.",
                "type": [
                  "integer",
                  "null"
                ],
                "format": "int64"
              },
              "value": {
                "description": "Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string.",
                "type": [
                  "string",
                  "null"
                ]
              }
            }
          }
        }
      }
    },
    "operatorLogLevel": {
      "description": "operatorLogLevel controls the logging level of the DNS Operator. Valid values are: \"Normal\", \"Debug\", \"Trace\". Defaults to \"Normal\". setting operatorLogLevel: Trace will produce extremely verbose logs.",
      "type": [
        "string",
        "null"
      ]
    },
    "servers": {
      "description": "servers is a list of DNS resolvers that provide name query delegation for one or more subdomains outside the scope of the cluster domain. If servers consists of more than one Server, longest suffix match will be used to determine the Server.\n\nFor example, if there are two Servers, one for \"foo.com\" and another for \"a.foo.com\", and the name query is for \"www.a.foo.com\", it will be routed to the Server with Zone \"a.foo.com\".\n\nIf this field is nil, no servers are created.",
      "type": [
        "array",
        "null"
      ],
      "items": {
        "description": "Server defines the schema for a server that runs per instance of CoreDNS.",
        "type": [
          "object",
          "null"
        ],
        "required": [
          "name",
          "zones",
          "forwardPlugin"
        ],
        "properties": {
          "forwardPlugin": {
            "description": "ForwardPlugin defines a schema for configuring the CoreDNS forward plugin.",
            "type": "object",
            "required": [
              "upstreams"
            ],
            "properties": {
              "policy": {
                "description": "policy is used to determine the order in which upstream servers are selected for querying. Any one of the following values may be specified:\n\n* \"Random\" picks a random upstream server for each query. * \"RoundRobin\" picks upstream servers in a round-robin order, moving to the next server for each new query. * \"Sequential\" tries querying upstream servers in a sequential order until one responds, starting with the first server for each new query.\n\nThe default value is \"Random\"",
                "type": [
                  "string",
                  "null"
                ]
              },
              "transportConfig": {
                "description": "DNSTransportConfig groups related configuration parameters used for configuring forwarding to upstream resolvers that support DNS-over-TLS.",
                "type": [
                  "object",
                  "null"
                ],
                "properties": {
                  "tls": {
                    "description": "DNSOverTLSConfig describes optional DNSTransportConfig fields that should be captured.",
                    "type": [
                      "object",
                      "null"
                    ],
                    "required": [
                      "serverName"
                    ],
                    "properties": {
                      "caBundle": {
                        "description": "ConfigMapNameReference references a config map in a specific namespace. The namespace must be specified at the point of use.",
                        "type": [
                          "object",
                          "null"
                        ],
                        "required": [
                          "name"
                        ],
                        "properties": {
                          "name": {
                            "description": "name is the metadata.name of the referenced config map",
                            "type": "string",
                            "default": ""
                          }
                        }
                      },
                      "serverName": {
                        "description": "serverName is the upstream server to connect to when forwarding DNS queries. This is required when Transport is set to \"TLS\". ServerName will be validated against the DNS naming conventions in RFC 1123 and should match the TLS certificate installed in the upstream resolver(s).",
                        "type": "string",
                        "default": ""
                      }
                    }
                  },
                  "transport": {
                    "description": "transport allows cluster administrators to opt-in to using a DNS-over-TLS connection between cluster DNS and an upstream resolver(s). Configuring TLS as the transport at this level without configuring a CABundle will result in the system certificates being used to verify the serving certificate of the upstream resolver(s).\n\nPossible values: \"\" (empty) - This means no explicit choice has been made and the platform chooses the default which is subject to change over time. The current default is \"Cleartext\". \"Cleartext\" - Cluster admin specified cleartext option. This results in the same functionality as an empty value but may be useful when a cluster admin wants to be more explicit about the transport, or wants to switch from \"TLS\" to \"Cleartext\" explicitly. \"TLS\" - This indicates that DNS queries should be sent over a TLS connection. If Transport is set to TLS, you MUST also set ServerName. If a port is not included with the upstream IP, port 853 will be tried by default per RFC 7858 section 3.1; https://datatracker.ietf.org/doc/html/rfc7858#section-3.1.",
                    "type": [
                      "string",
                      "null"
                    ]
                  }
                },
                "x-kubernetes-unions": [
                  {
                    "discriminator": "transport",
                    "fields-to-discriminateBy": {
                      "tls": "TLS"
                    }
                  }
                ]
              },
              "upstreams": {
                "description": "upstreams is a list of resolvers to forward name queries for subdomains of Zones. Each instance of CoreDNS performs health checking of Upstreams. When a healthy upstream returns an error during the exchange, another resolver is tried from Upstreams. The Upstreams are selected in the order specified in Policy. Each upstream is represented by an IP address or IP:port if the upstream listens on a port other than 53.\n\nA maximum of 15 upstreams is allowed per ForwardPlugin.",
                "type": "array",
                "items": {
                  "type": [
                    "string",
                    "null"
                  ],
                  "default": ""
                }
              }
            }
          },
          "name": {
            "description": "name is required and specifies a unique name for the server. Name must comply with the Service Name Syntax of rfc6335.",
            "type": "string",
            "default": ""
          },
          "zones": {
            "description": "zones is required and specifies the subdomains that Server is authoritative for. Zones must conform to the rfc1123 definition of a subdomain. Specifying the cluster domain (i.e., \"cluster.local\") is invalid.",
            "type": "array",
            "items": {
              "type": [
                "string",
                "null"
              ],
              "default": ""
            }
          }
        }
      }
    },
    "upstreamResolvers": {
      "description": "UpstreamResolvers defines a schema for configuring the CoreDNS forward plugin in the specific case of the default (\".\") server. It defers from ForwardPlugin in the default values it accepts: * At least one upstream should be specified. * the default policy is Sequential",
      "type": [
        "object",
        "null"
      ],
      "properties": {
        "policy": {
          "description": "Policy is used to determine the order in which upstream servers are selected for querying. Any one of the following values may be specified:\n\n* \"Random\" picks a random upstream server for each query. * \"RoundRobin\" picks upstream servers in a round-robin order, moving to the next server for each new query. * \"Sequential\" tries querying upstream servers in a sequential order until one responds, starting with the first server for each new query.\n\nThe default value is \"Sequential\"",
          "type": [
            "string",
            "null"
          ]
        },
        "transportConfig": {
          "description": "DNSTransportConfig groups related configuration parameters used for configuring forwarding to upstream resolvers that support DNS-over-TLS.",
          "type": [
            "object",
            "null"
          ],
          "properties": {
            "tls": {
              "description": "DNSOverTLSConfig describes optional DNSTransportConfig fields that should be captured.",
              "type": [
                "object",
                "null"
              ],
              "required": [
                "serverName"
              ],
              "properties": {
                "caBundle": {
                  "description": "ConfigMapNameReference references a config map in a specific namespace. The namespace must be specified at the point of use.",
                  "type": [
                    "object",
                    "null"
                  ],
                  "required": [
                    "name"
                  ],
                  "properties": {
                    "name": {
                      "description": "name is the metadata.name of the referenced config map",
                      "type": "string",
                      "default": ""
                    }
                  }
                },
                "serverName": {
                  "description": "serverName is the upstream server to connect to when forwarding DNS queries. This is required when Transport is set to \"TLS\". ServerName will be validated against the DNS naming conventions in RFC 1123 and should match the TLS certificate installed in the upstream resolver(s).",
                  "type": "string",
                  "default": ""
                }
              }
            },
            "transport": {
              "description": "transport allows cluster administrators to opt-in to using a DNS-over-TLS connection between cluster DNS and an upstream resolver(s). Configuring TLS as the transport at this level without configuring a CABundle will result in the system certificates being used to verify the serving certificate of the upstream resolver(s).\n\nPossible values: \"\" (empty) - This means no explicit choice has been made and the platform chooses the default which is subject to change over time. The current default is \"Cleartext\". \"Cleartext\" - Cluster admin specified cleartext option. This results in the same functionality as an empty value but may be useful when a cluster admin wants to be more explicit about the transport, or wants to switch from \"TLS\" to \"Cleartext\" explicitly. \"TLS\" - This indicates that DNS queries should be sent over a TLS connection. If Transport is set to TLS, you MUST also set ServerName. If a port is not included with the upstream IP, port 853 will be tried by default per RFC 7858 section 3.1; https://datatracker.ietf.org/doc/html/rfc7858#section-3.1.",
              "type": [
                "string",
                "null"
              ]
            }
          },
          "x-kubernetes-unions": [
            {
              "discriminator": "transport",
              "fields-to-discriminateBy": {
                "tls": "TLS"
              }
            }
          ]
        },
        "upstreams": {
          "description": "Upstreams is a list of resolvers to forward name queries for the \".\" domain. Each instance of CoreDNS performs health checking of Upstreams. When a healthy upstream returns an error during the exchange, another resolver is tried from Upstreams. The Upstreams are selected in the order specified in Policy.\n\nA maximum of 15 upstreams is allowed per ForwardPlugin. If no Upstreams are specified, /etc/resolv.conf is used by default",
          "type": [
            "array",
            "null"
          ],
          "items": {
            "description": "Upstream can either be of type SystemResolvConf, or of type Network.\n\n* For an Upstream of type SystemResolvConf, no further fields are necessary:\n  The upstream will be configured to use /etc/resolv.conf.\n* For an Upstream of type Network, a NetworkResolver field needs to be defined\n  with an IP address or IP:port if the upstream listens on a port other than 53.",
            "type": [
              "object",
              "null"
            ],
            "required": [
              "type"
            ],
            "properties": {
              "address": {
                "description": "Address must be defined when Type is set to Network. It will be ignored otherwise. It must be a valid ipv4 or ipv6 address.",
                "type": [
                  "string",
                  "null"
                ]
              },
              "port": {
                "description": "Port may be defined when Type is set to Network. It will be ignored otherwise. Port must be between 65535",
                "type": [
                  "integer",
                  "null"
                ],
                "format": "int64"
              },
              "type": {
                "description": "Type defines whether this upstream contains an IP/IP:port resolver or the local /etc/resolv.conf. Type accepts 2 possible values: SystemResolvConf or Network.\n\n* When SystemResolvConf is used, the Upstream structure does not require any further fields to be defined:\n  /etc/resolv.conf will be used\n* When Network is used, the Upstream structure must contain at least an Address",
                "type": "string",
                "default": ""
              }
            }
          }
        }
      }
    }
  },
  "$schema": "http://json-schema.org/schema#"
}